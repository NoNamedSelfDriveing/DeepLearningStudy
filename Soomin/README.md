# Index
[1. 퍼셉트론](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/1_Perceptron.md)

[2. 신경망과 활성화 함수](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/2_ActFunction.md)

[3. 행렬곱과 신경망](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/3_MatrixDotOperation.md)

[4. 행렬곱을 이용한 다층 신경망](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/4_MultiLayerNetwork.md)

[5. SOFTMAX 함수](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/5_Softmax.md)

[6. MNIST 데이터셋](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/6_MNISTdataset.md)

[7. 신경망의 학습과 손실 함수](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/7_TrainingNetwork.md)

[8. 경사 하강법](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/8_GradientDescentMethod.md)

[9. 선형 회귀법](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/9_LinearRegression.md)

[10. 이중 분류의 구현](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/10_BInaryClassfication.md)

[11. SOFTMAX 분류의 구현](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/11_SOFTMAXclassfication.md)

[12. 역전파](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/12_Backpropagation.md)

[13. MNIST 분류의 구현](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/13_MNISTclassfication.md)

[14. XOR 문제 구현](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/14_XORproblem.md)

[15. Deep and Wide XOR](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/15_DeepAndWideXOR.md)

[16. 다층 신경망을 이용한 MNIST 분류](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/16_MNISTusingSimpleNN.md)

[17. 데이터의 전처리](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/17_DataPreProcessing.md)

[18. Overfitting(과적합)의 방지](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/18_Overfitting.md)

[19. 가중치(Weight)의 초기화 방법](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/19_Weight_Initialization.md)

[20. 매개변수 최적화 알고리즘 1 : SGD, Momentum, NAG](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/20_Optimizer1.md)

[21. 매개변수 최적화 알고리즘 2 : AdaGrad, RMSProp, Adam](https://github.com/MagmaTart/DeepLearningStudy/blob/master/Soomin/summarys/21_Optimizer2.md)

# First Mindgaim...

딥 러닝을 당장 프로젝트를 하기 위해서 급하게 공부했었다. 이 과정에서 깊은 원리에 대한 지식 없이 Tensorflow 라이브러리 사용만을 했던 것 같은데, 이 레포지토리와 함께 딥 러닝을 처음부터 다시, 천천히 그리고 깊이 공부하려고 한다.

도서는 「Deep Learning from Scratch」(O'REILLY, 2016)을 사용한다. 이후에 사용할 라이브러리는 Tensorflow와 Keras를 먼저 공부하여 이용하겠다.

Tensorflow 실습을 위한 내용과 스터디 참고 자료로, 김성훈 교수님의 [모두를 위한 딥러닝 강의(2016)](http://hunkim.github.io/ml/) 를 활용하도록 하겠다.

또 많은 자료는 Stanford University의 [CS231n](http://cs231n.stanford.edu/syllabus.html) 강좌를 이용하였다.

또 이 내용은 [내 블로그](https://blog.naver.com/leesoo9297)에도 옮겨 다시 정리하고 있다.